- [X] add prefix to training
- [ ] visualize attention
- [X] add batch normalization to GAP layer
- [X] make batch normalization deactivatable
- [ ] replace Dense with CNN
- [ ] what would batch normalization do in our case?
- [X] draw picture of transform layer
- [X] draw picture of GAPNet
- [ ] draw picture of multihead attention layer
- [ ] make transform layer
- [ ] let model yield transformation output
- [ ] see if their CNN stuff is good or bad
- [ ] what about the (undocumented) skip connection point_cloud_expanded?
- [ ] https://www.tensorflow.org/guide/upgrade
- [ ] what is this? https://github.com/TianzhongSong/PointNet-Keras/blob/master/callbacks.py Change of LR
- [ ] make sure that trainable parameters are properly displayed in summary()
- [ ] implement load/save
- [ ] write detailed documentation
- [ ] consider commenting on GAPNet implementation details, thoughts and findings
- [ ]
- [ ]
- [ ]
- [ ]



- [X] write README, with references
- [X] publish
- [X] make assertions for all those shapes
- [X] make assertions for second attention single and multi
- [X] consider squeezing attention coefficients in GAP
- [X] what is tf.tile(input_feature, [1, 1, k, 1]) in gat_layers?
- [X] use this for training https://github.com/TianzhongSong/PointNet-Keras
- [X] make build model work
- [X] get attentions out of multihead attention
- [X] use 1024 instead of 2048 as number_of_points
- [X] make assertions for first attention single and multi
- [X] move attention to own layer
- [X] move multihead attention to own layer
